# STEP 4-2
==========

현재의 로직에서 서버가 스케일 아웃되면, replica DB 를 도입하게 되면 생기는 문제를 고민해주세요.
-------------

***

## 목차🧭

- [개요](#개요)
- [현재 상황](#현재-상황)
- [To-Be 상황](#도입-후)
- [주요 문제점](#주요-문제점)
- [해결 방안](#해결-방안)
***

## 개요📜

>서버가 스케일 아웃되고 레플리카 db 를 도입할 때 발생할 수 있는 문제점들을 검토합니다.

***   

## 현재 상황🖼️

>현재는 단일 서버와 단일 db 구성으로 운영되고 있습니다.  
>모든 읽기/쓰기 작업이 하나의 db 에서 처리됩니다.  
***

## 도입 후🔄

>서버가 부하에 따라 scale in/out 되면서, 여러 서버가 동시에 동작할 수 있습니다.  
>db 레플리카를 도입하여, 읽기 작업을 분산하여 처리할 수 있습니다.

## 문제점과 해결안

| 문제점                                                                 | 해결안                                                                                                                                                                             |
|---------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Replication lag**: Master와 Replica 간 데이터 동기화 지연으로 인한 데이터 불일치 발생 가능성 | 서비스의 특성 상 지연이 너무 큰 상황은 없을 것으로 예상되어, 최종적 일관성 모델을 적용   <br/>(적정수준을 넘어서는 지연시간에 대한 모니터링 및 개선은 주기적으로 진행할 필요가 있습니다.)                                                                  |
| **분산 트랜잭션**: 여러 서버에 걸친 트랜잭션에 대한 일관성 보장이 어려움| 현재로써는 트랜잭션의 범위가 서버 내로 한정된다고 판단하여 별도의 처리는 진행하지 않을 예정 <br/> (수익 모델 적용 등의 이유로 트랜잭션 범위가 확장되는 경우 분산 처리 패턴 적용을 고려할 수 있습니다.)                                                           
| **Key 충돌**: 각 서버가 독립적으로 생성하는 Key 값의 충돌 발생 가능성 존재| UUID, ULID, TSID, snowflake ID 등의 key 생성 전략 적용 <br/> -> 현재 서비스가 크지 않고, key 의 길이를 가능한 한 짧게 유지할 생각이므로, TSID 를 활용할 예정입니다.      <br/> (물론 서비스가 성장함에 따라 key 생성 전략을 언제든지 변경할 수 있습니다.) |
| **로그 분산**: 여러 서버에 분산된 로그의 통합 분석이 복잡해짐| 각 서버별로 발생하는 로그에 대한 trace ID, machine ID 등을 통해 통합 로깅 및 분석 <br/> -> observability 스텝에서 피드백 받은 내용을 토대로 오류 추적에 대한 추가 정보를 대시보드를 통해 제공할 예정입니다.                                        |
| **DB Connection**: 읽기/쓰기 분리에 따른 애플리케이션 코드 변경 가능성| CQRS 패턴 적용 고려 <br/> -> 오버 엔지니어링으로 느껴져, 초기에는 트랜잭션 readonly 속성에 따른 datasource routing 를 통해 구현할 예정입니다.                                                                             |
| **분산 락(Lock)**: 동시성 제어를 위한 분산 락 메커니즘 구현이 필요해질 수 있음| 현재로써는 특별히 고려되지 않아도 되는 내용 <br/> (추후 수정, 갱신 등의 기능이 추가되는 경우 적용을 고려하겠습니다.)                                                                                                          |
| **배포 프로세스**: 여러 서버에 걸친 배포 자동화 및 롤백 전략 필요| rolling-update 등의 배포 전략 적용. scale in/out에 대해 alp(application load balancer), auto-scaling 적용  <br/> -> 고가용성을 위해 최소 2개의 가용영역에 인스턴스가 1개 이상 존재할 수 있도록 운영할 계획입니다.                   |
| **설정 관리**: 서버별 설정의 일관성 유지 및 중앙화된 관리 필요| machine image, launch template 등을 통한 공통된 서버 설정 관리. s3 혹은 efs nas 연결 방식을 통한 설정 파일 공유 <br/> -> ami snapshot 을 위한 인스턴스를 추가로 생성할 예정입니다. <br/>(최초 생성 및 설정 갱신때만 on-demand 방식으로 이용)    |